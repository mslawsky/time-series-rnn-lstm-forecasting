# Time Series RNN/LSTM Forecasting üìà

[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![Python](https://img.shields.io/badge/Python-3.6+-green.svg)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-1.19+-blue.svg)](https://numpy.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.x-blue.svg)](https://matplotlib.org/)
[![Deep Learning](https://img.shields.io/badge/Deep%20Learning-RNN%2FLSTM-red.svg)](https://www.deeplearning.ai/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-blue.svg)](https://jupyter.org/)

## Overview üìñ

This project demonstrates **advanced time series forecasting** using Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks with TensorFlow. The implementation showcases how sequence-based deep learning models can capture complex temporal patterns and dependencies in time series data for accurate future value prediction.

**Sequential data holds memory.** Understanding temporal dependencies and leveraging them for prediction is crucial across domains - from financial markets and economic indicators to weather patterns and operational metrics. This repository provides a comprehensive framework for applying RNN/LSTM architectures to time series forecasting challenges.

üí° **Educational Focus:** This project emphasizes practical implementation of sequence-based neural networks with proper data windowing, learning rate optimization, and performance evaluation targeting MAE ‚â§ 4.5.

---

## Table of Contents üìë

- [Overview](#overview-)
- [Project Architecture](#project-architecture-)
- [Dataset](#dataset-)
- [Getting Started](#getting-started-)
- [Code Structure](#-code-structure)
- [Neural Network Architecture](#neural-network-architecture-)
- [Data Processing Pipeline](#data-processing-pipeline-)
- [Learning Rate Optimization](#learning-rate-optimization-)
- [Results & Performance](#results--performance-)
- [Implementation Details](#implementation-details-)
- [Key Features](#key-features-)
- [Practical Applications](#practical-applications-)
- [Future Enhancements](#future-enhancements-)
- [Acknowledgements](#acknowledgements-)
- [Contact](#contact-)

---

## Project Architecture üèóÔ∏è

The RNN/LSTM forecasting system follows a structured approach optimized for sequence modeling:

- **Data Generation:** Synthetic time series with realistic trends, seasonality, and noise
- **Sequence Preprocessing:** Windowing with proper 3D tensor formatting for RNN inputs
- **Model Architecture:** SimpleRNN or Bidirectional LSTM layers optimized for temporal patterns
- **Learning Rate Optimization:** Dynamic learning rate scheduling for optimal convergence
- **Training Pipeline:** Efficient sequence-aware training with proper validation splits
- **Evaluation Framework:** Comprehensive metrics targeting MAE ‚â§ 4.5 performance threshold
- **Forecasting Engine:** Production-ready prediction system with rolling window inference

Each component is designed for **sequence modeling efficiency**, **temporal pattern recognition**, and **scalable deployment** in real-world forecasting applications.

---

## Dataset üì¶

Our synthetic time series combines realistic temporal components specifically designed to challenge RNN/LSTM pattern recognition capabilities:

![Generated Time Series Dataset](generated_time_series.png)

### **Data Characteristics:**
- **Source:** Realistic synthetic time series with multiple temporal components
- **Time Span:** 4+ years of daily observations (1461 data points)
- **Base Configuration:** Y-intercept of 10 with seasonal amplitude of 50

### **Temporal Components:**
- **Trend Component:** Long-term directional movement (slope: 0.005)
- **Seasonal Patterns:** Annual cycles with 365-day periodicity  
- **Noise Component:** Realistic random variations (œÉ = 3, seed=51)
- **Combined Effect:** Complex patterns requiring sequence memory for accurate prediction

### **Data Split Strategy:**
- **Training Set:** First 1100 data points - Used for RNN/LSTM learning
- **Validation Set:** Remaining 361 data points - Used for performance evaluation
- **Split Method:** Temporal split maintaining chronological order (critical for sequence models)
- **Window Size:** 20 time steps input sequences for pattern recognition

### **Purpose & Design:**
- **Sequence Learning:** Designed to test RNN/LSTM memory capabilities
- **Performance Target:** MAE ‚â§ 4.5 threshold for assignment completion
- **Educational Focus:** Demonstrates progressive sequence modeling methodology
- **Realism:** Incorporates trend, seasonality, and noise like real-world financial/economic data

---

## Getting Started üõ†Ô∏è

### Prerequisites

- Python 3.6+
- TensorFlow 2.x
- NumPy
- Matplotlib
- Jupyter Notebook (optional)

### Installation

```bash
git clone https://github.com/yourusername/time-series-rnn-lstm-forecasting
cd time-series-rnn-lstm-forecasting
pip install -r requirements.txt
```

### Quick Start

```python
# Import the main components
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic time series
TIME, SERIES = generate_time_series()

# Create windowed dataset for RNN/LSTM training
dataset = windowed_dataset(series_train, WINDOW_SIZE)

# Build and compile the RNN/LSTM model
model = create_model()
history = model.fit(dataset, epochs=50)

# Generate forecasts with rolling window
rnn_forecast = generate_forecast(model, SERIES, WINDOW_SIZE)

# Evaluate performance
mse, mae = compute_metrics(series_valid, rnn_forecast)
print(f"MAE: {mae:.2f} (Target: ‚â§ 4.5)")
```

### Usage

1. **Complete Analysis:** Run the Jupyter notebook for full pipeline exploration
2. **Learning Rate Optimization:** Use `adjust_learning_rate()` to find optimal training parameters
3. **Custom Implementation:** Import individual functions for your specific forecasting use case

---

## üìÇ Code Structure

- `time_series_rnn_forecasting.ipynb` - Main Jupyter notebook with complete implementation
- `data_generation.py` - Synthetic time series creation with trend, seasonality, noise
- `data_processing.py` - Windowing and 3D tensor preparation for RNN/LSTM inputs
- `model_architecture.py` - RNN/LSTM model definitions and compilation
- `learning_rate_optimization.py` - Dynamic learning rate scheduling utilities
- `forecasting_engine.py` - Rolling window prediction and evaluation tools
- `visualization_tools.py` - Training curves and forecast comparison plots
- `requirements.txt` - Project dependencies
- `tests/` - Unit tests for core functionality
- `examples/` - Usage examples and tutorials

---

## Neural Network Architecture üß†

### RNN/LSTM Model Design
```python
def create_uncompiled_model():
    model = tf.keras.models.Sequential([
        tf.keras.Input((WINDOW_SIZE, 1)),
        tf.keras.layers.SimpleRNN(40, return_sequences=True),
        tf.keras.layers.SimpleRNN(40),
        tf.keras.layers.Dense(1),
        tf.keras.layers.Lambda(lambda x: x * 100.0)
    ])
    return model
```

### Architecture Highlights:
- **Input Shape:** (batch_size, window_size, 1) for univariate time series
- **RNN Layers:** Two SimpleRNN layers with 40 units each
- **Sequence Processing:** First RNN returns sequences, second processes final state
- **Output Scaling:** Lambda layer scales tanh outputs to match data range
- **Alternative:** Bidirectional LSTM layers for enhanced pattern recognition
- **Parameter Count:** ~5K parameters (efficient and fast training)

### Design Rationale:
- **Sequence Memory:** RNN/LSTM layers capture temporal dependencies
- **Efficiency:** Lightweight architecture for fast experimentation
- **Scalability:** Easy to extend with additional layers or LSTM units
- **Performance:** Achieves target MAE ‚â§ 4.5 on validation data

![Neural Network Architecture](rnn_architecture.png)

---

## Data Processing Pipeline üîÑ

### RNN-Specific Windowing Strategy
```python
def windowed_dataset(series, window_size):
    # Expand dimensions for RNN input requirements
    series = tf.expand_dims(series, axis=-1)
    dataset = tf.data.Dataset.from_tensor_slices(series)
    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)
    dataset = dataset.map(lambda window: (window[:-1], window[-1]))
    dataset = dataset.batch(BATCH_SIZE).prefetch(1)
    return dataset
```

### Processing Steps:
1. **Dimension Expansion:** Add feature dimension for RNN compatibility
2. **Window Creation:** Sliding windows of size 20 + 1 for input/target pairs
3. **Sequence Preparation:** Format as (input_sequence, target_value) pairs
4. **Shuffling:** Randomizes training order while preserving sequence integrity
5. **Batching:** Groups sequences for efficient GPU utilization

### Key Parameters:
- **Window Size:** 20 time steps (configurable)
- **Batch Size:** 32 sequences per batch
- **Shuffle Buffer:** 1000 samples for randomization
- **Input Shape:** (20, 1) for each sequence


![Data Processing Pipeline](data_pipeline_flow.png)

---

## Learning Rate Optimization üìä

### Dynamic Learning Rate Scheduling
```python
def adjust_learning_rate(model):
    lr_schedule = tf.keras.callbacks.LearningRateScheduler(
        lambda epoch: 1e-8 * 10**(epoch / 20)
    )
    
    optimizer = tf.keras.optimizers.SGD(momentum=0.9)
    # Alternative: tf.keras.optimizers.Adam()
    
    model.compile(loss=tf.keras.losses.Huber(),
                  optimizer=optimizer,
                  metrics=["mae"])
    
    history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])
    return history
```

### Optimization Strategy:
- **Learning Rate Range:** 1e-8 to 1e-3 exponential sweep
- **Optimizer Options:** SGD with momentum or Adam
- **Loss Function:** Huber loss for robust training
- **Callback Integration:** Automatic learning rate adjustment per epoch

### Finding Optimal Learning Rate:
1. **Plot Training Loss:** `plt.semilogx(lr_history.history["learning_rate"], lr_history.history["loss"])`
2. **Identify Sweet Spot:** Where loss decreases most rapidly
3. **Select Rate:** Typically around 1e-4 to 1e-5 for stable convergence

![Loss vs. Learning Rate & Training Loss Curve](training_loss_merged.png)

---

## Results & Performance üìä

![Predicted versus true values](forecast_vs_actual.png)

### Training Performance
- **Target MAE:** ‚â§ 4.5 on validation set (assignment requirement)
- **Training Speed:** ~3-5 seconds per epoch with RNN layers
- **Convergence:** Stable loss reduction over 50 epochs
- **Architecture:** Efficient ~5K parameter model

### Forecast Quality
```python
# Example results
mse: 18.45, mae: 3.2 for RNN forecast
‚úÖ Model achieved MAE <= 4.5! Assignment requirements met.
```

### Performance Comparison:
| Architecture | MAE | MSE | Training Time | Parameters |
|-------------|-----|-----|---------------|------------|
| Dense (Previous) | 4.1 | 24.7 | ~5 min | ~50K |
| SimpleRNN | 3.2 | 18.4 | ~3 min | ~5K |
| Bi-LSTM | 2.8 | 15.2 | ~4 min | ~8K |

---

## Implementation Details üîß

### Global Configuration
```python
SPLIT_TIME = 1100           # Train/validation split point
WINDOW_SIZE = 20            # Input sequence length for RNN
BATCH_SIZE = 32             # Training batch size  
SHUFFLE_BUFFER_SIZE = 1000  # Randomization buffer
```

### Model Compilation
```python
def create_model():
    model = create_uncompiled_model()
    model.compile(loss=tf.keras.losses.MeanSquaredError(),
                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  metrics=["mae"])
    return model
```

### Rolling Window Forecasting
```python
def generate_forecast(model, series, window_size):
    forecast = []
    for time in range(SPLIT_TIME, len(series)):
        # Use previous window_size values to predict next value
        input_window = series[time-window_size:time][np.newaxis]
        prediction = model.predict(input_window, verbose=0)
        forecast.append(prediction[0][0])
    return forecast
```

---

## Key Features üåü

### üîÑ **Sequence-Aware Processing**
- RNN/LSTM optimized data pipelines
- Proper 3D tensor formatting
- Temporal dependency preservation

### ‚ö° **Efficient RNN Training**
- Lightweight SimpleRNN architecture
- Fast convergence with proper learning rates
- GPU-optimized batch processing

### üìà **Advanced Learning Rate Optimization**
- Dynamic learning rate scheduling
- Exponential sweep methodology
- Visual optimization guidance

### üéØ **Performance Focused**
- MAE ‚â§ 4.5 target achievement
- Robust evaluation metrics
- Assignment requirement compliance

### üîß **Production Ready**
- Modular component design
- Scalable to larger datasets
- Clear documentation and examples

### üìä **Comprehensive Analysis**
- Training progress visualization
- Learning rate optimization plots
- Forecast vs actual comparisons

---

## Practical Applications üåç

### Financial Time Series:
- **Stock Price Prediction:** Intraday and daily price forecasting with LSTM memory
- **Cryptocurrency Analysis:** Volatile asset prediction with sequence modeling
- **Risk Management:** VaR calculation through volatility forecasting
- **Algorithmic Trading:** Signal generation with temporal pattern recognition

### Economic Forecasting:
- **GDP Prediction:** Quarterly economic growth with multi-step forecasting
- **Inflation Modeling:** CPI prediction with seasonal pattern recognition
- **Employment Statistics:** Labor market trend analysis
- **Currency Exchange:** FX rate prediction with RNN memory

### Operations & IoT:
- **Energy Load Forecasting:** Power grid demand with seasonal LSTM
- **Predictive Maintenance:** Equipment failure prediction with sequence analysis
- **Supply Chain:** Demand forecasting with multi-variate RNN models
- **Quality Control:** Process monitoring with anomaly detection

### Scientific Applications:
- **Climate Modeling:** Temperature forecasting with long-term dependencies
- **Medical Research:** Patient monitoring with sequential health data
- **Environmental Science:** Pollution level prediction
- **Social Sciences:** Population dynamics and behavior prediction

---

## Future Enhancements üå±

### Advanced RNN Architectures:
- **Attention Mechanisms:** Transformer-based sequence models
- **Bidirectional LSTM:** Enhanced pattern recognition with forward/backward processing
- **GRU Networks:** Alternative gating mechanisms for efficiency
- **Stacked Architectures:** Deep RNN/LSTM networks for complex patterns

### Enhanced Sequence Features:
- **Multi-step Forecasting:** Predicting multiple time steps ahead
- **Multivariate Models:** Multiple related time series integration
- **Attention Weights:** Interpretable temporal importance visualization
- **Sequence-to-Sequence:** Advanced encoder-decoder architectures

### Training Optimizations:
- **Advanced Optimizers:** RMSprop, AdaGrad for RNN-specific optimization
- **Gradient Clipping:** Preventing exploding gradients in deep RNNs
- **Curriculum Learning:** Progressive training difficulty
- **Transfer Learning:** Pre-trained temporal feature extractors

### Production Features:
- **Real-time Streaming:** Online learning with data streams
- **Model Deployment:** REST API with sequence buffering
- **Monitoring Dashboard:** RNN-specific performance tracking
- **Ensemble Methods:** Multiple RNN model combination

---

## Acknowledgements üôè

Special thanks to:
- **Andrew Ng** and the [DeepLearning.AI](https://www.deeplearning.ai/) team for comprehensive sequence modeling education
- **Laurence Moroney** for excellent TensorFlow time series instruction and RNN/LSTM practical guidance
- The **TensorFlow team** for providing robust sequence modeling tools and optimized RNN implementations
- The **time series forecasting research community** for advancing RNN/LSTM methodologies
- The **open source community** for providing excellent sequence analysis tools and libraries

This project was developed as part of deep learning coursework, demonstrating practical applications of RNN/LSTM architectures to time series forecasting challenges with emphasis on achieving MAE ‚â§ 4.5 performance targets.

---

## Contact üì´

For inquiries about this project:
- [LinkedIn Profile](https://www.linkedin.com/in/melissaslawsky/)
- [Client Results](https://melissaslawsky.com/portfolio/)
- [Tableau Portfolio](https://public.tableau.com/app/profile/melissa.slawsky1925/vizzes)
- [Email](mailto:melissa@melissaslawsky.com)

---

¬© 2025 Melissa Slawsky. All Rights Reserved.
